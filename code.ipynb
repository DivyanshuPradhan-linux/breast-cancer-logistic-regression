import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np

# Step 1: Choose a binary classification dataset (using Breast Cancer Wisconsin Dataset as an example)
# You can download the dataset using the link provided in the document [cite: 5]
# Assuming the dataset is saved as 'wdbc.data' and is in the same directory
# The dataset columns are described here: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names
# We will use columns 2-31 as features and column 1 as the target.

# Load the dataset
# The dataset does not have a header, so we provide column names
column_names = ['ID', 'Diagnosis'] + [f'feature_{i}' for i in range(1, 31)]
data = pd.read_csv('wdbc.data', names=column_names)

# Separate features (X) and target (y)
# 'M' for malignant, 'B' for benign
X = data.iloc[:, 2:]
y = data['Diagnosis'].map({'M': 1, 'B': 0})

# Step 2: Train/test split and standardize features
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 3: Fit a Logistic Regression model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Step 4: Evaluate with confusion matrix, precision, recall, ROC-AUC
y_pred = model.predict(X_test_scaled)
y_prob = model.predict_proba(X_test_scaled)[:, 1] # Probability of the positive class

conf_matrix = confusion_matrix(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

print("Confusion Matrix:\n", conf_matrix)
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"ROC-AUC: {roc_auc:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure()
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Step 5: Tune threshold and explain sigmoid function
# The default threshold is 0.5. We can tune it based on the specific requirements (e.g., prioritize recall over precision).
# Example of tuning the threshold to prioritize recall:
# Find the threshold that gives a desired recall (e.g., 0.95)
# You would typically use a validation set for this, but for demonstration, we use the test set
# You would iterate through thresholds and find the one that meets your criteria.

# Explanation of Sigmoid Function:
# The sigmoid function (also known as the logistic function) is a mathematical function that maps any real-valued number to a value between 0 and 1.
# In logistic regression, the sigmoid function is used to transform the linear output of the model into a probability.
# The formula for the sigmoid function is: S(x) = 1 / (1 + e^(-x)), where 'x' is the linear output (dot product of weights and features plus bias).
# This output can be interpreted as the probability of the instance belonging to the positive class.
# A threshold is then applied to this probability to make a binary classification decision. If the probability is above the threshold, the instance is classified as positive; otherwise, it is classified as negative.

print("\nExplanation of Sigmoid Function:")
print("The sigmoid function maps any real-valued number to a value between 0 and 1. In logistic regression, it converts the linear output into a probability.")
print("Formula: S(x) = 1 / (1 + e^(-x))")
print("A threshold is applied to this probability for binary classification.")

# Example of tuning the threshold (visualizing how precision and recall change with threshold)
thresholds = np.linspace(0, 1, 100)
precisions = []
recalls = []

for threshold in thresholds:
    y_pred_threshold = (y_prob >= threshold).astype(int)
    precisions.append(precision_score(y_test, y_pred_threshold, zero_division=0))
    recalls.append(recall_score(y_test, y_pred_threshold))

plt.figure()
plt.plot(thresholds, precisions, label='Precision')
plt.plot(thresholds, recalls, label='Recall')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('Precision and Recall vs. Threshold')
plt.legend()
plt.show()
